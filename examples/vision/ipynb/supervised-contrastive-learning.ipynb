{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engineerjkk/Supervised-Contrastive-Learning/blob/main/examples/vision/ipynb/supervised-contrastive-learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA9zhPpQu-hc"
      },
      "source": [
        "# Supervised Contrastive Learning\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2020/11/30<br>\n",
        "**Last modified:** 2020/11/30<br>\n",
        "**Description:** Using supervised contrastive learning for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q29mDsyu-hf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "(Prannay Khosla et al.) is a training methodology that outperforms\n",
        "supervised training with crossentropy on classification tasks.\n",
        "\n",
        "Essentially, training an image classification model with Supervised Contrastive\n",
        "Learning is performed in two phases:\n",
        "\n",
        "1. Training an encoder to learn to produce vector representations of input images such\n",
        "that representations of images in the same class will be more similar compared to\n",
        "representations of images in different classes.\n",
        "2. Training a classifier on top of the frozen encoder.\n",
        "\n",
        "Note that this example requires [TensorFlow Addons](https://www.tensorflow.org/addons), which you can install using the following command:\n",
        "\n",
        "```python\n",
        "pip install tensorflow-addons\n",
        "```\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy2RLiWfwlJM",
        "outputId": "06142390-3b9a-428d-ebbb-7ecb11762115"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A5omkTD1u-hh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvuXg-ufu-hi"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR10 데이터셋을 사용한다.  \n",
        "클래스 수는 10개이며, 32 by 32 pixels의 3채널을 갖는 컬러이미지 데이터셋이다. "
      ],
      "metadata": {
        "id": "wRy8e23GAtMa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUf3nCjPu-hi",
        "outputId": "7ecd09a2-1bde-4037-fc2c-715db97ef865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "# Load the train and test data splits\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Display shapes of train and test datasets\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0])"
      ],
      "metadata": {
        "id": "5k86Qy83BHk2",
        "outputId": "711939ce-5ec9-4cfe-92d7-a7f8d84da891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5793051a50>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MTTd7YIYu-hj"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),#랜덤으로 가로로 뒤집어줌.\n",
        "        layers.RandomRotation(0.02),#랜덤으로 0.02만큼 Rotation을 수행해줌.\n",
        "        layers.RandomWidth(0.2),#width를 0.2만큼 늘려준다.\n",
        "        layers.RandomHeight(0.2),#Height를 0.2만큼 늘려준다. \n",
        "    ]\n",
        ")\n",
        "\n",
        "# Setting the state of the normalization layer.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4TGeNJXu-hk"
      },
      "source": [
        "## Build the encoder model\n",
        "\n",
        "The encoder model takes the image as input and turns it into a 2048-dimensional\n",
        "feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH6qktIzu-hk",
        "outputId": "e157dcaf-e0eb-4d05-d3a6-809ad3fbb7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 32, 32, 3)         7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,564,807\n",
            "Trainable params: 23,519,360\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_encoder():\n",
        "    #ResNet 내부 파라미터를 설정한다. include_top을 False를 함으로써 FC Layer를 안줘서 Feature Task만 사용한다.\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
        "    )\n",
        "    \n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)#augmented된 이미지를 넣어준다. \n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
        "    return model\n",
        "\n",
        "\n",
        "encoder = create_encoder()#create_encoder를 통해서 모델을 만든다. \n",
        "encoder.summary()#아래 encoder에 대한 summary가 출력된다.\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 265\n",
        "hidden_units = 512\n",
        "projection_units = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.5\n",
        "temperature = 0.05\n",
        "#32 by 32 사이즈의 3채널 이미지가 input으로 들어오게되며, 최종 출력되는 차원은 2048 dimension의 feature vector가 출력된다.\n",
        "#Fully Connected Layer가 없으므로 2048차원의 feature vector만 사용되는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrQH78FWu-hl"
      },
      "source": [
        "## Build the classification model\n",
        "\n",
        "The classification model adds a fully-connected layer on top of the encoder,\n",
        "plus a softmax layer with the target classes.\n",
        "\n",
        "이제 Stage2의 Classification model을 만들어야한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "19cdiaFIu-hl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_classifier(encoder, trainable=True):\n",
        "\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    print(\"input_shape :\",input_shape)\n",
        "    features = encoder(inputs)#input shape 32x32x3 사이즈를 넣어준다.\n",
        "    features = layers.Dropout(dropout_rate)(features)#드롭아웃과함께 이전 레이어의 아웃풋을 넣어준다.\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)#128차원으로 만들어주며, activation function은 relu를 사용한다.\n",
        "    features = layers.Dropout(dropout_rate)(features)#다시한번 더 dropout을 사용한다. \n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)#class 개수만큼의 차원으로 만들어준다. activation function은 softmax를 사용한다.\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")#이렇게 classifier 모델을 완성해준다.\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),#optimizer는 Adan을 사용했으며, learning rate는 0.001을 사용한다.\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),#사용하는 cross entropy로는 sparseCetegorialCrossentropy를 사용한다. 내부적으로 알아서 one hot vector로 만들어주는 cross entropy이다. \n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgBqI_GKu-hm"
      },
      "source": [
        "## Experiment 1: Train the baseline classification model\n",
        "\n",
        "In this experiment, a baseline classifier is trained as usual, i.e., the\n",
        "encoder and the classifier parts are trained together as a single model\n",
        "to minimize the crossentropy loss.\n",
        "\n",
        "encoder와 classifier 부분이 함께 하나의 모델에서 train되며, crossentropy loss를 minimize한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJedHe7wu-hm",
        "outputId": "d6b7ebce-89a8-465f-c191-7be8560f8eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,619,025\n",
            "Trainable params: 24,573,578\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "189/189 [==============================] - 70s 259ms/step - loss: 1.9271 - sparse_categorical_accuracy: 0.2902\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 25s 132ms/step - loss: 1.5275 - sparse_categorical_accuracy: 0.4478\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 30s 161ms/step - loss: 1.3979 - sparse_categorical_accuracy: 0.5009\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 25s 133ms/step - loss: 1.2628 - sparse_categorical_accuracy: 0.5565\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 28s 150ms/step - loss: 1.1551 - sparse_categorical_accuracy: 0.5987\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 24s 129ms/step - loss: 1.0665 - sparse_categorical_accuracy: 0.6296\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 29s 153ms/step - loss: 1.0018 - sparse_categorical_accuracy: 0.6539\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 24s 128ms/step - loss: 0.9471 - sparse_categorical_accuracy: 0.6731\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.9338 - sparse_categorical_accuracy: 0.6797\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.8562 - sparse_categorical_accuracy: 0.7085\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 0.8345 - sparse_categorical_accuracy: 0.7157\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 0.7968 - sparse_categorical_accuracy: 0.7302\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 0.7803 - sparse_categorical_accuracy: 0.7352\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.7172 - sparse_categorical_accuracy: 0.7534\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.7111 - sparse_categorical_accuracy: 0.7584\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.7581 - sparse_categorical_accuracy: 0.7452\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.7723\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 24s 128ms/step - loss: 0.8727 - sparse_categorical_accuracy: 0.7001\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 24s 124ms/step - loss: 0.7379 - sparse_categorical_accuracy: 0.7480\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 0.6490 - sparse_categorical_accuracy: 0.7797\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.6049 - sparse_categorical_accuracy: 0.7954\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.5888 - sparse_categorical_accuracy: 0.7992\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.6084 - sparse_categorical_accuracy: 0.7947\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.5573 - sparse_categorical_accuracy: 0.8114\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.5358 - sparse_categorical_accuracy: 0.8193\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 24s 129ms/step - loss: 0.5150 - sparse_categorical_accuracy: 0.8253\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.5034 - sparse_categorical_accuracy: 0.8286\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 24s 128ms/step - loss: 0.4811 - sparse_categorical_accuracy: 0.8364\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.4741 - sparse_categorical_accuracy: 0.8392\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.5023 - sparse_categorical_accuracy: 0.8293\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.5168 - sparse_categorical_accuracy: 0.8269\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 0.4469 - sparse_categorical_accuracy: 0.8474\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 24s 130ms/step - loss: 0.4323 - sparse_categorical_accuracy: 0.8517\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.4189 - sparse_categorical_accuracy: 0.8567\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.4084 - sparse_categorical_accuracy: 0.8600\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.3998 - sparse_categorical_accuracy: 0.8652\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 0.3849 - sparse_categorical_accuracy: 0.8676\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 0.3735 - sparse_categorical_accuracy: 0.8738\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.3682 - sparse_categorical_accuracy: 0.8747\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 0.3501 - sparse_categorical_accuracy: 0.8802\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.3476 - sparse_categorical_accuracy: 0.8812\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.3349 - sparse_categorical_accuracy: 0.8837\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.3234 - sparse_categorical_accuracy: 0.8889\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 0.3162 - sparse_categorical_accuracy: 0.8911\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.3114 - sparse_categorical_accuracy: 0.8926\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 0.3056 - sparse_categorical_accuracy: 0.8941\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.7566 - sparse_categorical_accuracy: 0.7593\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 0.7375 - sparse_categorical_accuracy: 0.7454\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 0.4367 - sparse_categorical_accuracy: 0.8510\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 0.3534 - sparse_categorical_accuracy: 0.8790\n",
            "313/313 [==============================] - 6s 13ms/step - loss: 0.7713 - sparse_categorical_accuracy: 0.7986\n",
            "Test accuracy: 79.86%\n"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder()#encoder 모델을 생성해주며,\n",
        "classifier = create_classifier(encoder)#인코더 모델의 output을 classifier에 넣어준다. \n",
        "classifier.summary()\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-cwVbkKu-ho"
      },
      "source": [
        "## Experiment 2: Use supervised contrastive learning\n",
        "\n",
        "In this experiment, the model is trained in two phases. In the first phase,\n",
        "the encoder is pretrained to optimize the supervised contrastive loss,\n",
        "described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n",
        "\n",
        "In the second phase, the classifier is trained using the trained encoder with\n",
        "its weights freezed; only the weights of fully-connected layers with the\n",
        "softmax are optimized.\n",
        "\n",
        "### 1. Supervised contrastive learning loss function\n",
        "Loss Function을 제안한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B1qn216cu-hp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature #similarity 값을 scaling해주는 tau인 temperature 값을 1로 설정한다. \n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)##veature vector를 Normalization 시켜준뒤에,\n",
        "        # Compute logits \n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)#자기 자신과 autmented된 positive 또는 negative image를 곱해준다.\n",
        "            ),\n",
        "            self.temperature,#그리고 scaling을 위해 temperature constant로 나누어준다. \n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits) #tfa.losses.npairs_loss API를 사용해서 실제 레이블된 ground true 값과 예측된 값의 pair loss를 계산한다. \n",
        "\n",
        "\n",
        "def add_projection_head(encoder):#인코더 이후에 projection head를 추가한다. MLP이다.\n",
        "    inputs = keras.Input(shape=input_shape)#3채널 32 by 32의 input shape을 input으로 넣어준뒤,\n",
        "    features = encoder(inputs)#인코더에 넣어준다.\n",
        "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)#그리고 DenseLayer로 128차원으로 만들어주고, ReLU Activation Function을 사용한다. \n",
        "    model = keras.Model(#이렇게 input 3채널 32 by 32 사이즈 input, 128차원의 output으로 나오는 모델을 생성하여준다.\n",
        "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjFh7bRu-hp"
      },
      "source": [
        "### 2. Pretrain the encoder\n",
        "- 먼저 pretraining과정이 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBNGkRVVu-hq",
        "outputId": "69f83216-2ec5-4f4a-8c3d-e4f2c8e4f0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               262272    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,827,079\n",
            "Trainable params: 23,781,632\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "189/189 [==============================] - 29s 125ms/step - loss: 5.3751\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 24s 130ms/step - loss: 5.1291\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 24s 124ms/step - loss: 5.0108\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 4.9164\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 4.8123\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 4.7326\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 4.6674\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 24s 128ms/step - loss: 4.6036\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 4.5448\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 4.5012\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 4.4553\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 4.4145\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 4.3872\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 4.3487\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 4.3052\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 4.2786\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 4.2491\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 4.2172\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 4.2065\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 4.1612\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 4.1597\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 4.1401\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 4.1060\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 4.0795\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 4.0666\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 4.0547\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 4.0359\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 4.0098\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 4.0045\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 3.9737\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 3.9722\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 3.9404\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 3.9401\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 3.9254\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 3.9111\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 3.8906\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 3.8883\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 3.8780\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 3.8570\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 3.8523\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 3.8382\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 3.8271\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 3.8169\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 3.7997\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 24s 125ms/step - loss: 3.7952\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 3.7860\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 24s 126ms/step - loss: 3.7666\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 3.7633\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 3.7622\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 23s 124ms/step - loss: 3.7681\n"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder() #인코더를 생성해준다. \n",
        "\n",
        "encoder_with_projection_head = add_projection_head(encoder)#그리고 인코더에 projection head를 추가해준다. \n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),#optimizer로는 Adam optimizer를 사용하며,\n",
        "    loss=SupervisedContrastiveLoss(temperature),#Loss Function으로는 SupCon LossFunction을 사용하며, 파라미터인 temperature constant 1을 줍니다. \n",
        ")\n",
        "\n",
        "encoder_with_projection_head.summary()#Summary를 해주고,\n",
        "\n",
        "history = encoder_with_projection_head.fit(#x에 트레이닝셋, y에 y 트레이닝셋, batch_size는 265를 주었으며, epochs는 50을 주어 학습을 진행했습니다. \n",
        "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5myPWPUu-hq"
      },
      "source": [
        "### 3. Train the classifier with the frozen encoder\n",
        "- encoder를 frozen시킨 값을 classifier에 넣어줘서 classifier를 생성한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssaozG-Du-hq",
        "outputId": "12070e00-0fa9-4470-9dbb-3fb26930c44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "189/189 [==============================] - 10s 37ms/step - loss: 0.3583 - sparse_categorical_accuracy: 0.9031\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2975 - sparse_categorical_accuracy: 0.9111\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2871 - sparse_categorical_accuracy: 0.9115\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2809 - sparse_categorical_accuracy: 0.9114\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2878 - sparse_categorical_accuracy: 0.9120\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 7s 38ms/step - loss: 0.2797 - sparse_categorical_accuracy: 0.9129\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2791 - sparse_categorical_accuracy: 0.9128\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2761 - sparse_categorical_accuracy: 0.9138\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2754 - sparse_categorical_accuracy: 0.9136\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2858 - sparse_categorical_accuracy: 0.9102\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2840 - sparse_categorical_accuracy: 0.9098\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2823 - sparse_categorical_accuracy: 0.9129\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2806 - sparse_categorical_accuracy: 0.9125\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2688 - sparse_categorical_accuracy: 0.9170\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2732 - sparse_categorical_accuracy: 0.9151\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2853 - sparse_categorical_accuracy: 0.9105\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2807 - sparse_categorical_accuracy: 0.9121\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2682 - sparse_categorical_accuracy: 0.9162\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2775 - sparse_categorical_accuracy: 0.9140\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2768 - sparse_categorical_accuracy: 0.9136\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 7s 38ms/step - loss: 0.2763 - sparse_categorical_accuracy: 0.9141\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2741 - sparse_categorical_accuracy: 0.9144\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2759 - sparse_categorical_accuracy: 0.9136\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2829 - sparse_categorical_accuracy: 0.9111\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2795 - sparse_categorical_accuracy: 0.9124\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2836 - sparse_categorical_accuracy: 0.9115\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2784 - sparse_categorical_accuracy: 0.9118\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2744 - sparse_categorical_accuracy: 0.9153\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2765 - sparse_categorical_accuracy: 0.9116\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2853 - sparse_categorical_accuracy: 0.9109\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 8s 41ms/step - loss: 0.2729 - sparse_categorical_accuracy: 0.9139\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.2752 - sparse_categorical_accuracy: 0.9142\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.2715 - sparse_categorical_accuracy: 0.9141\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 8s 40ms/step - loss: 0.2797 - sparse_categorical_accuracy: 0.9124\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 7s 38ms/step - loss: 0.2684 - sparse_categorical_accuracy: 0.9150\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 8s 43ms/step - loss: 0.2749 - sparse_categorical_accuracy: 0.9134\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 8s 40ms/step - loss: 0.2769 - sparse_categorical_accuracy: 0.9129\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 7s 39ms/step - loss: 0.2773 - sparse_categorical_accuracy: 0.9117\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 8s 41ms/step - loss: 0.2743 - sparse_categorical_accuracy: 0.9132\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 7s 39ms/step - loss: 0.2744 - sparse_categorical_accuracy: 0.9135\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 7s 39ms/step - loss: 0.2762 - sparse_categorical_accuracy: 0.9137\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 7s 39ms/step - loss: 0.2822 - sparse_categorical_accuracy: 0.9107\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 7s 38ms/step - loss: 0.2759 - sparse_categorical_accuracy: 0.9119\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2731 - sparse_categorical_accuracy: 0.9140\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 7s 39ms/step - loss: 0.2764 - sparse_categorical_accuracy: 0.9141\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2767 - sparse_categorical_accuracy: 0.9130\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2751 - sparse_categorical_accuracy: 0.9127\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2740 - sparse_categorical_accuracy: 0.9130\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2683 - sparse_categorical_accuracy: 0.9166\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2746 - sparse_categorical_accuracy: 0.9134\n",
            "313/313 [==============================] - 5s 13ms/step - loss: 0.7127 - sparse_categorical_accuracy: 0.8168\n",
            "Test accuracy: 81.68%\n"
          ]
        }
      ],
      "source": [
        "classifier = create_classifier(encoder, trainable=False)#\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "#classifier 모델의 fit에 x train과 y train 그리고 batch size는 256을 주었으며 epoch는 50회를 주었다. \n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNkgWMo6u-hr"
      },
      "source": [
        "We get to an improved test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NIkPzddu-hr"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "As shown in the experiments, using the supervised contrastive learning technique\n",
        "outperformed the conventional technique in terms of the test accuracy. Note that\n",
        "the same training budget (i.e., number of epochs) was given to each technique.\n",
        "Supervised contrastive learning pays off when the encoder involves a complex\n",
        "architecture, like ResNet, and multi-class problems with many labels.\n",
        "In addition, large batch sizes and multi-layer projection heads\n",
        "improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "paper for more details.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/supervised-contrastive-learning-cifar10) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/supervised-contrastive-learning)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GInMN8qpwaC2"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "supervised-contrastive-learning",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}