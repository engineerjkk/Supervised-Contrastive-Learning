{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engineerjkk/Supervised-Contrastive-Learning/blob/main/examples/vision/ipynb/supervised-contrastive-learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA9zhPpQu-hc"
      },
      "source": [
        "# Supervised Contrastive Learning\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2020/11/30<br>\n",
        "**Last modified:** 2020/11/30<br>\n",
        "**Description:** Using supervised contrastive learning for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q29mDsyu-hf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "(Prannay Khosla et al.) is a training methodology that outperforms\n",
        "supervised training with crossentropy on classification tasks.\n",
        "\n",
        "Essentially, training an image classification model with Supervised Contrastive\n",
        "Learning is performed in two phases:\n",
        "\n",
        "1. Training an encoder to learn to produce vector representations of input images such\n",
        "that representations of images in the same class will be more similar compared to\n",
        "representations of images in different classes.\n",
        "2. Training a classifier on top of the frozen encoder.\n",
        "\n",
        "Note that this example requires [TensorFlow Addons](https://www.tensorflow.org/addons), which you can install using the following command:\n",
        "\n",
        "```python\n",
        "pip install tensorflow-addons\n",
        "```\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "Jy2RLiWfwlJM",
        "outputId": "d33af155-a682-4e4a-a563-918e2330d848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A5omkTD1u-hh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvuXg-ufu-hi"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EUf3nCjPu-hi",
        "outputId": "7d472e3c-d69e-4359-89ed-b405ed3df630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 14s 0us/step\n",
            "170508288/170498071 [==============================] - 14s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "# Load the train and test data splits\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Display shapes of train and test datasets\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tULOSm9ju-hj"
      },
      "source": [
        "## Using image data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MTTd7YIYu-hj"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.02),\n",
        "        layers.RandomWidth(0.2),\n",
        "        layers.RandomHeight(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Setting the state of the normalization layer.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4TGeNJXu-hk"
      },
      "source": [
        "## Build the encoder model\n",
        "\n",
        "The encoder model takes the image as input and turns it into a 2048-dimensional\n",
        "feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uH6qktIzu-hk",
        "outputId": "15748902-1e18-411c-b168-cb9e5035baef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 32, 32, 3)         7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,564,807\n",
            "Trainable params: 23,519,360\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_encoder():\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
        "    )\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
        "    return model\n",
        "\n",
        "\n",
        "encoder = create_encoder()\n",
        "encoder.summary()\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 265\n",
        "hidden_units = 512\n",
        "projection_units = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.5\n",
        "temperature = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrQH78FWu-hl"
      },
      "source": [
        "## Build the classification model\n",
        "\n",
        "The classification model adds a fully-connected layer on top of the encoder,\n",
        "plus a softmax layer with the target classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "19cdiaFIu-hl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_classifier(encoder, trainable=True):\n",
        "\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgBqI_GKu-hm"
      },
      "source": [
        "## Experiment 1: Train the baseline classification model\n",
        "\n",
        "In this experiment, a baseline classifier is trained as usual, i.e., the\n",
        "encoder and the classifier parts are trained together as a single model\n",
        "to minimize the crossentropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FJedHe7wu-hm",
        "outputId": "eb86cf08-7ca6-4499-966c-076412a4c11d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,619,025\n",
            "Trainable params: 24,573,578\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "189/189 [==============================] - 69s 248ms/step - loss: 1.9603 - sparse_categorical_accuracy: 0.2776\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 28s 148ms/step - loss: 1.5258 - sparse_categorical_accuracy: 0.4436\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 24s 127ms/step - loss: 1.3358 - sparse_categorical_accuracy: 0.5210\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 1.2202 - sparse_categorical_accuracy: 0.5703\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 1.1150 - sparse_categorical_accuracy: 0.6118\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 1.0403 - sparse_categorical_accuracy: 0.6404\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 26s 137ms/step - loss: 1.0185 - sparse_categorical_accuracy: 0.6513\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 0.9296 - sparse_categorical_accuracy: 0.6812\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.8635 - sparse_categorical_accuracy: 0.7033\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.8133 - sparse_categorical_accuracy: 0.7214\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 0.8012 - sparse_categorical_accuracy: 0.7276\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 27s 142ms/step - loss: 0.8117 - sparse_categorical_accuracy: 0.7245\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.7513 - sparse_categorical_accuracy: 0.7451\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.6982 - sparse_categorical_accuracy: 0.7626\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.6682 - sparse_categorical_accuracy: 0.7721\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 0.6475 - sparse_categorical_accuracy: 0.7764\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.6291 - sparse_categorical_accuracy: 0.7847\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 0.6125 - sparse_categorical_accuracy: 0.7925\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 0.6418 - sparse_categorical_accuracy: 0.7814\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.5803 - sparse_categorical_accuracy: 0.8037\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.5499 - sparse_categorical_accuracy: 0.8127\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 0.5283 - sparse_categorical_accuracy: 0.8197\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.5181 - sparse_categorical_accuracy: 0.8225\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 0.5057 - sparse_categorical_accuracy: 0.8269\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.4936 - sparse_categorical_accuracy: 0.8317\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 0.4847 - sparse_categorical_accuracy: 0.8358\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 0.8099 - sparse_categorical_accuracy: 0.7356\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.7274 - sparse_categorical_accuracy: 0.7544\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.5691 - sparse_categorical_accuracy: 0.8064\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.4986 - sparse_categorical_accuracy: 0.8319\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 0.4623 - sparse_categorical_accuracy: 0.8437\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.4402 - sparse_categorical_accuracy: 0.8487\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 0.4231 - sparse_categorical_accuracy: 0.8563\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.4055 - sparse_categorical_accuracy: 0.8590\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.3939 - sparse_categorical_accuracy: 0.8658\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.7587 - sparse_categorical_accuracy: 0.7463\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.5334 - sparse_categorical_accuracy: 0.8200\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 0.5758 - sparse_categorical_accuracy: 0.8091\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 0.4339 - sparse_categorical_accuracy: 0.8527\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.3941 - sparse_categorical_accuracy: 0.8640\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.3720 - sparse_categorical_accuracy: 0.8738\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 0.3546 - sparse_categorical_accuracy: 0.8794\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 0.3859 - sparse_categorical_accuracy: 0.8696\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 23s 123ms/step - loss: 0.3768 - sparse_categorical_accuracy: 0.8695\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 22s 116ms/step - loss: 0.3352 - sparse_categorical_accuracy: 0.8857\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.3240 - sparse_categorical_accuracy: 0.8896\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 0.3031 - sparse_categorical_accuracy: 0.8953\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 0.3016 - sparse_categorical_accuracy: 0.8968\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 0.3061 - sparse_categorical_accuracy: 0.8957\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 0.2881 - sparse_categorical_accuracy: 0.8999\n",
            "313/313 [==============================] - 6s 14ms/step - loss: 0.9571 - sparse_categorical_accuracy: 0.7808\n",
            "Test accuracy: 78.08%\n"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder()\n",
        "classifier = create_classifier(encoder)\n",
        "classifier.summary()\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-cwVbkKu-ho"
      },
      "source": [
        "## Experiment 2: Use supervised contrastive learning\n",
        "\n",
        "In this experiment, the model is trained in two phases. In the first phase,\n",
        "the encoder is pretrained to optimize the supervised contrastive loss,\n",
        "described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n",
        "\n",
        "In the second phase, the classifier is trained using the trained encoder with\n",
        "its weights freezed; only the weights of fully-connected layers with the\n",
        "softmax are optimized.\n",
        "\n",
        "### 1. Supervised contrastive learning loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B1qn216cu-hp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
        "\n",
        "\n",
        "def add_projection_head(encoder):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjFh7bRu-hp"
      },
      "source": [
        "### 2. Pretrain the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KBNGkRVVu-hq",
        "outputId": "f90eb36d-9227-4110-d8d3-284b42a79219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               262272    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,827,079\n",
            "Trainable params: 23,781,632\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "189/189 [==============================] - 27s 118ms/step - loss: 5.3615\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 5.1494\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 5.0326\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.9286\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.8283\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 4.7570\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.6924\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 4.6187\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 4.5753\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.5130\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.4795\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 4.4321\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 24s 128ms/step - loss: 4.4066\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 4.3526\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 4.3331\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 22s 116ms/step - loss: 4.3042\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 4.2693\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.2390\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.2192\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 4.1932\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 23s 122ms/step - loss: 4.1690\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 4.1639\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 4.1338\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 4.1066\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 4.0779\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 4.0737\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 4.0559\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 4.0448\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 4.0236\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 4.0131\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 3.9884\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 3.9806\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 22s 116ms/step - loss: 3.9668\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 3.9363\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 3.9308\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 3.9112\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 3.9087\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 3.8910\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 3.8925\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 3.8744\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 22s 117ms/step - loss: 3.8628\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 3.8475\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 3.8346\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 3.8193\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 3.8155\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 23s 121ms/step - loss: 3.8109\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 22s 119ms/step - loss: 3.7957\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 23s 119ms/step - loss: 3.7787\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 23s 120ms/step - loss: 3.7780\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 22s 118ms/step - loss: 3.7629\n"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder()\n",
        "\n",
        "encoder_with_projection_head = add_projection_head(encoder)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature),\n",
        ")\n",
        "\n",
        "encoder_with_projection_head.summary()\n",
        "\n",
        "history = encoder_with_projection_head.fit(\n",
        "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5myPWPUu-hq"
      },
      "source": [
        "### 3. Train the classifier with the frozen encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssaozG-Du-hq",
        "outputId": "19bcf360-c337-4202-d069-afe8d5695837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "189/189 [==============================] - 10s 35ms/step - loss: 0.3588 - sparse_categorical_accuracy: 0.9021\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.3173 - sparse_categorical_accuracy: 0.9048\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2963 - sparse_categorical_accuracy: 0.9097\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2894 - sparse_categorical_accuracy: 0.9112\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2951 - sparse_categorical_accuracy: 0.9089\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2839 - sparse_categorical_accuracy: 0.9121\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2839 - sparse_categorical_accuracy: 0.9122\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2912 - sparse_categorical_accuracy: 0.9092\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2947 - sparse_categorical_accuracy: 0.9077\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.3014 - sparse_categorical_accuracy: 0.9067\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 6s 34ms/step - loss: 0.2875 - sparse_categorical_accuracy: 0.9105\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2878 - sparse_categorical_accuracy: 0.9112\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 7s 34ms/step - loss: 0.2895 - sparse_categorical_accuracy: 0.9098\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2943 - sparse_categorical_accuracy: 0.9094\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2854 - sparse_categorical_accuracy: 0.9101\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2917 - sparse_categorical_accuracy: 0.9092\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 7s 34ms/step - loss: 0.2877 - sparse_categorical_accuracy: 0.9096\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2882 - sparse_categorical_accuracy: 0.9108\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2755 - sparse_categorical_accuracy: 0.9141\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2794 - sparse_categorical_accuracy: 0.9122\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2904 - sparse_categorical_accuracy: 0.9082\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2806 - sparse_categorical_accuracy: 0.9116\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2878 - sparse_categorical_accuracy: 0.9103\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2861 - sparse_categorical_accuracy: 0.9117\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2902 - sparse_categorical_accuracy: 0.9088\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 6s 34ms/step - loss: 0.2899 - sparse_categorical_accuracy: 0.9100\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2904 - sparse_categorical_accuracy: 0.9092\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2866 - sparse_categorical_accuracy: 0.9090\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2837 - sparse_categorical_accuracy: 0.9110\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2863 - sparse_categorical_accuracy: 0.9087\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2853 - sparse_categorical_accuracy: 0.9103\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 7s 37ms/step - loss: 0.2951 - sparse_categorical_accuracy: 0.9077\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2869 - sparse_categorical_accuracy: 0.9095\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.3028 - sparse_categorical_accuracy: 0.9059\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2853 - sparse_categorical_accuracy: 0.9086\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2852 - sparse_categorical_accuracy: 0.9110\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 7s 36ms/step - loss: 0.2845 - sparse_categorical_accuracy: 0.9116\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2828 - sparse_categorical_accuracy: 0.9111\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2774 - sparse_categorical_accuracy: 0.9152\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2855 - sparse_categorical_accuracy: 0.9116\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2946 - sparse_categorical_accuracy: 0.9071\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2830 - sparse_categorical_accuracy: 0.9123\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2801 - sparse_categorical_accuracy: 0.9115\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.9087\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2915 - sparse_categorical_accuracy: 0.9085\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.9099\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 7s 34ms/step - loss: 0.2870 - sparse_categorical_accuracy: 0.9105\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2911 - sparse_categorical_accuracy: 0.9099\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2893 - sparse_categorical_accuracy: 0.9078\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 7s 35ms/step - loss: 0.2856 - sparse_categorical_accuracy: 0.9114\n"
          ]
        }
      ],
      "source": [
        "classifier = create_classifier(encoder, trainable=False)\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNkgWMo6u-hr"
      },
      "source": [
        "We get to an improved test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NIkPzddu-hr"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "As shown in the experiments, using the supervised contrastive learning technique\n",
        "outperformed the conventional technique in terms of the test accuracy. Note that\n",
        "the same training budget (i.e., number of epochs) was given to each technique.\n",
        "Supervised contrastive learning pays off when the encoder involves a complex\n",
        "architecture, like ResNet, and multi-class problems with many labels.\n",
        "In addition, large batch sizes and multi-layer projection heads\n",
        "improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "paper for more details.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/supervised-contrastive-learning-cifar10) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/supervised-contrastive-learning)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GInMN8qpwaC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "supervised-contrastive-learning",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}